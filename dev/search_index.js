var documenterSearchIndex = {"docs":
[{"location":"tutorials/schedule_coordination/#Tutorial:-Schedule-Coordination-with-COHDA","page":"Schedule Coordination (COHDA)","title":"Tutorial: Schedule Coordination with COHDA","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"This tutorial shows how to use COHDA to coordinate a group of participants that each select a power schedule from a menu of options, with the goal of matching a target aggregate profile.","category":"page"},{"location":"tutorials/schedule_coordination/#Problem-Statement","page":"Schedule Coordination (COHDA)","title":"Problem Statement","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"Four distributed energy units (e.g., controllable loads) participate in a demand-response event. The grid operator asks for an aggregate power profile of [3.0, 2.0, 1.0] kW over three time steps.","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"Each unit can only operate in one of several predefined modes (schedules). The task is to find a combination — one schedule per unit — whose sum is as close as possible to the target.","category":"page"},{"location":"tutorials/schedule_coordination/#Step-1-—-Define-the-Participants","page":"Schedule Coordination (COHDA)","title":"Step 1 — Define the Participants","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"Each participant is created with create_cohda_participant. Arguments:","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"id — unique integer identifier\nschedules — vector of candidate schedules (each a Vector{Float64} of the same length as the target)","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"using DistributedResourceOptimization\n\n# Unit A: can be OFF, run at half power, or run at full power\nschedules_A = [\n    [0.0, 0.0, 0.0],   # OFF\n    [1.0, 0.5, 0.0],   # half power\n    [2.0, 1.0, 0.5],   # full power\n]\n\n# Unit B: similar options but different profile shapes\nschedules_B = [\n    [0.0, 0.0, 0.0],\n    [0.5, 1.0, 0.5],\n    [1.0, 1.0, 0.5],\n]\n\nunit_A1 = create_cohda_participant(1, schedules_A)\nunit_A2 = create_cohda_participant(2, schedules_A)\nunit_B1 = create_cohda_participant(3, schedules_B)\nunit_B2 = create_cohda_participant(4, schedules_B)","category":"page"},{"location":"tutorials/schedule_coordination/#Step-2-—-Create-the-Start-Message","page":"Schedule Coordination (COHDA)","title":"Step 2 — Create the Start Message","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"The start message carries the target profile that all participants try to match collectively:","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"target    = [3.0, 2.0, 1.0]\nstart_msg = create_cohda_start_message(target)","category":"page"},{"location":"tutorials/schedule_coordination/#Step-3-—-Run-the-Distributed-Optimization","page":"Schedule Coordination (COHDA)","title":"Step 3 — Run the Distributed Optimization","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"wait(start_distributed_optimization(\n    [unit_A1, unit_A2, unit_B1, unit_B2],\n    start_msg,\n))","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"start_distributed_optimization uses a SimpleCarrier internally. The call returns a task; wait blocks until the algorithm converges.","category":"page"},{"location":"tutorials/schedule_coordination/#Step-4-—-Inspect-the-Result","page":"Schedule Coordination (COHDA)","title":"Step 4 — Inspect the Result","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"After convergence, the selected schedule for each participant is stored in its WorkingMemory. You can access it via the algorithm's internal state:","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"# The memory field holds the best found system configuration\nwm = unit_A1.memory\n\n# The solution candidate holds the selected schedules as a Matrix (rows = participants)\nprintln(\"Best performance: \", wm.solution_candidate.perf)\n\n# Extract the aggregate (sum over participants) of the chosen schedules\naggregate = vec(sum(wm.solution_candidate.schedules, dims=1))\nprintln(\"Aggregate schedule: \", aggregate)\nprintln(\"Target:             \", target)\nprintln(\"L1 deviation:       \", sum(abs.(aggregate .- target)))","category":"page"},{"location":"tutorials/schedule_coordination/#Complete-Script","page":"Schedule Coordination (COHDA)","title":"Complete Script","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"using DistributedResourceOptimization\n\n# Schedule menus\nschedules_A = [[0.0, 0.0, 0.0], [1.0, 0.5, 0.0], [2.0, 1.0, 0.5]]\nschedules_B = [[0.0, 0.0, 0.0], [0.5, 1.0, 0.5], [1.0, 1.0, 0.5]]\n\n# Participants\nparticipants = [\n    create_cohda_participant(1, schedules_A),\n    create_cohda_participant(2, schedules_A),\n    create_cohda_participant(3, schedules_B),\n    create_cohda_participant(4, schedules_B),\n]\n\n# Target and start\ntarget    = [3.0, 2.0, 1.0]\nstart_msg = create_cohda_start_message(target)\n\n# Solve\nwait(start_distributed_optimization(participants, start_msg))\n\n# Report\nwm        = participants[1].memory\naggregate = vec(sum(wm.solution_candidate.schedules, dims=1))\nprintln(\"Aggregate schedule: \", round.(aggregate; digits=3))\nprintln(\"Target:             \", target)\nprintln(\"L1 deviation:       \", round(sum(abs.(aggregate .- target)); digits=4))","category":"page"},{"location":"tutorials/schedule_coordination/#Using-SimpleCarrier-Directly","page":"Schedule Coordination (COHDA)","title":"Using SimpleCarrier Directly","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"If you need more control — for example, to inspect intermediate messages or integrate with a larger simulation — use SimpleCarrier explicitly:","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"using DistributedResourceOptimization\n\nschedules_A = [[0.0, 0.0, 0.0], [1.0, 0.5, 0.0], [2.0, 1.0, 0.5]]\n\ncontainer = ActorContainer()\ncarrier1  = SimpleCarrier(container, create_cohda_participant(1, schedules_A))\ncarrier2  = SimpleCarrier(container, create_cohda_participant(2, schedules_A))\n\nstart_msg = create_cohda_start_message([2.0, 1.0, 0.5])\n\n# Send start message from carrier1 to carrier2; returns a task\ntask = send_to_other(carrier1, start_msg, cid(carrier2))\nwait(task)","category":"page"},{"location":"tutorials/schedule_coordination/#Extending-with-Custom-Performance","page":"Schedule Coordination (COHDA)","title":"Extending with Custom Performance","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"By default COHDA uses a weighted L1 distance. Supply a custom performance function to create_cohda_participant to optimise a different objective. The function receives a Matrix{Float64} (rows = participants, columns = time steps) and a TargetParams struct:","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"using DistributedResourceOptimization\n\nschedules_A = [[0.0, 0.0, 0.0], [1.0, 0.5, 0.0], [2.0, 1.0, 0.5]]\n\n# Minimise L2 (Euclidean) distance instead of the default weighted L1\nmy_perf = (cluster_schedule, target_params) ->\n    -sqrt(sum((target_params.schedule .- vec(sum(cluster_schedule, dims=1))) .^ 2))\n\nunit = create_cohda_participant(1, (_) -> schedules_A, my_perf)","category":"page"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"note: Heuristic Quality\nCOHDA is a heuristic and may not find the global optimum, especially with many participants or a large schedule menu. In practice it converges quickly to near-optimal solutions on problems of realistic size (tens to hundreds of participants).","category":"page"},{"location":"tutorials/schedule_coordination/#Next-Steps","page":"Schedule Coordination (COHDA)","title":"Next Steps","text":"","category":"section"},{"location":"tutorials/schedule_coordination/","page":"Schedule Coordination (COHDA)","title":"Schedule Coordination (COHDA)","text":"See COHDA algorithm details for the mathematical background\nTry the Energy Dispatch Tutorial for the continuous ADMM variant\nSee How To: Implement a Custom Algorithm to add your own heuristic","category":"page"},{"location":"howtos/custom_carrier/#How-To:-Implement-a-Custom-Carrier","page":"Custom Carrier","title":"How To: Implement a Custom Carrier","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"A carrier is the communication layer that delivers messages between algorithm participants. DRO ships with SimpleCarrier (in-process) and MangoCarrier (Mango.jl TCP). This guide shows how to write your own.","category":"page"},{"location":"howtos/custom_carrier/#The-Interface","page":"Custom Carrier","title":"The Interface","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"All carriers extend Carrier and implement six functions:","category":"page"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"abstract type Carrier end\n\n# Send a message to another participant (fire-and-forget)\nsend_to_other(::Carrier, content::Any, receiver::Any)\n\n# Reply to the sender of an incoming message\nreply_to_other(::Carrier, content::Any, meta::Any)\n\n# Send a message and return an awaitable handle\nsend_awaitable(::Carrier, content::Any, receiver::Any)\n\n# Block until an awaitable handle completes; return its value\nBase.wait(::Carrier, waitable::Any)\n\n# Schedule a zero-argument function after `delay_s` seconds\nschedule_using(::Carrier, f::Function, delay_s::Float64)\n\n# Return IDs of all other participants (excluding self)\nothers(::Carrier, self_id::String)","category":"page"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"You do not need to implement get_address unless your carrier is used with parts of the codebase that call it explicitly.","category":"page"},{"location":"howtos/custom_carrier/#Step-by-Step-Example-—-Logging-Carrier","page":"Custom Carrier","title":"Step-by-Step Example — Logging Carrier","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"Here is a carrier that wraps SimpleCarrier and logs every message send. It is a useful debugging wrapper for any algorithm.","category":"page"},{"location":"howtos/custom_carrier/#.-Define-the-Struct","page":"Custom Carrier","title":"1. Define the Struct","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"using DistributedResourceOptimization\n\nstruct LoggingCarrier <: Carrier\n    inner::SimpleCarrier\nend","category":"page"},{"location":"howtos/custom_carrier/#.-Delegate-and-Log","page":"Custom Carrier","title":"2. Delegate and Log","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"Implement each required function by delegating to the inner carrier:","category":"page"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"function send_to_other(c::LoggingCarrier, content::Any, receiver::Any)\n    @info \"[LoggingCarrier] send → $receiver: $(typeof(content))\"\n    send_to_other(c.inner, content, receiver)\nend\n\nfunction reply_to_other(c::LoggingCarrier, content::Any, meta::Any)\n    sender = get(meta, :sender, \"?\")\n    @info \"[LoggingCarrier] reply → $sender: $(typeof(content))\"\n    reply_to_other(c.inner, content, meta)\nend\n\nfunction send_awaitable(c::LoggingCarrier, content::Any, receiver::Any)\n    @info \"[LoggingCarrier] send_awaitable → $receiver: $(typeof(content))\"\n    send_awaitable(c.inner, content, receiver)\nend\n\nfunction Base.wait(c::LoggingCarrier, waitable::Any)\n    wait(c.inner, waitable)\nend\n\nfunction schedule_using(c::LoggingCarrier, f::Function, delay_s::Float64)\n    schedule_using(c.inner, f, delay_s)\nend\n\nfunction others(c::LoggingCarrier, self_id::String)\n    others(c.inner, self_id)\nend","category":"page"},{"location":"howtos/custom_carrier/#.-Wire-It-Up","page":"Custom Carrier","title":"3. Wire It Up","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"Because algorithm dispatch happens through on_exchange_message(algo, carrier, msg, meta), you need to make sure your carrier is passed there. The easiest way is to subclass the relevant role or override dispatch — but for simple wrapping, you can construct LoggingCarrier around an existing SimpleCarrier:","category":"page"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"using DistributedResourceOptimization\n\ncontainer = ActorContainer()\n\nalgo1 = create_cohda_participant(1, [[0.0, 1.0], [1.0, 2.0]])\nalgo2 = create_cohda_participant(2, [[0.0, 1.0], [1.0, 2.0]])\n\nsc1 = SimpleCarrier(container, algo1)\nsc2 = SimpleCarrier(container, algo2)\n\nlc1 = LoggingCarrier(sc1)   # wraps carrier 1 for logging\n\nstart_msg = create_cohda_start_message([1.0, 2.0])\n\nwait(send_to_other(lc1, start_msg, cid(sc2)))","category":"page"},{"location":"howtos/custom_carrier/#Building-a-Carrier-from-Scratch","page":"Custom Carrier","title":"Building a Carrier from Scratch","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"If you want a completely independent carrier (e.g., backed by ZeroMQ or HTTP), follow these principles:","category":"page"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"Message dispatch — When a message arrives (from the network, a queue, etc.), call on_exchange_message(algorithm, your_carrier, message_data, meta).\nMetadata — The meta dict passed to on_exchange_message must contain at least :sender so that reply_to_other can route the response back. Additional keys are carrier-specific.\nAwaitable pattern — send_awaitable must return something that Base.wait can block on. The simplest implementation uses EventWithValue:\nfunction send_awaitable(c::MyCarrier, content::Any, receiver::Any)\n    event = EventWithValue(Base.Event(), nothing)\n    # store event keyed by a message ID; resolve it when the reply arrives\n    c.pending[my_id] = event\n    _dispatch_message(c, content, receiver, my_id)\n    return event\nend\n\nfunction Base.wait(c::MyCarrier, ev::EventWithValue)\n    wait(ev.event)\n    return ev.value\nend\nothers — Return a collection of addresses/IDs that the algorithm can pass back to send_to_other. The type is up to you; just be consistent.","category":"page"},{"location":"howtos/custom_carrier/#Checklist","page":"Custom Carrier","title":"Checklist","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"[ ] Subtype Carrier\n[ ] Implement send_to_other\n[ ] Implement reply_to_other\n[ ] Implement send_awaitable returning something Base.wait-able\n[ ] Implement Base.wait(carrier, waitable)\n[ ] Implement schedule_using\n[ ] Implement others\n[ ] Call on_exchange_message when an inbound message arrives","category":"page"},{"location":"howtos/custom_carrier/#See-Also","page":"Custom Carrier","title":"See Also","text":"","category":"section"},{"location":"howtos/custom_carrier/","page":"Custom Carrier","title":"Custom Carrier","text":"Carrier, send_to_other, reply_to_other\nsend_awaitable, schedule_using, others\nSimpleCarrier — reference implementation in src/carrier/simple.jl\nHow To: Implement a Custom Algorithm","category":"page"},{"location":"algorithms/consensus/#Averaging-Consensus","page":"Averaging Consensus","title":"Averaging Consensus","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"The Averaging Consensus algorithm distributes a parameter vector lambda across N agents via a gossip-style protocol. Each agent maintains its own copy of lambda and iteratively averages it with values received from neighbors. An optional gradient term allows each agent to steer the consensus towards locally desirable values.","category":"page"},{"location":"algorithms/consensus/#Algorithm","page":"Averaging Consensus","title":"Algorithm","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"Let lambda_i^k be the value held by agent i at iteration k. The update rule is:","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"lambda_i^k+1 = lambda_i^k + alpha left(barlambda^k - lambda_i^kright) + nabla_i^k","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"where","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"barlambda^k is the average of all received values at iteration k\nalpha in (01 is the step size (mixing parameter)\nnabla_i^k = textgradient_term(textactor_i lambda_i^k textdata) is the local gradient correction (zero by default)","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"The algorithm runs for a fixed number of iterations (max_iter) after which each agent calls a user-supplied finish_callback.","category":"page"},{"location":"algorithms/consensus/#Usage","page":"Averaging Consensus","title":"Usage","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"Create a participant with create_averaging_consensus_participant:","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"using DistributedResourceOptimization\n\nactor = create_averaging_consensus_participant(\n    # finish_callback: called with (algorithm, carrier) when max_iter is reached\n    (alg, _) -> println(\"Converged to λ = \", alg.λ),\n    NoConsensusActor();  # no local gradient correction\n    initial_λ = 10.0,   # starting value for all components\n    α         = 0.3,    # mixing step size\n    max_iter  = 50,     # number of gossip rounds\n)","category":"page"},{"location":"algorithms/consensus/#Parameters","page":"Averaging Consensus","title":"Parameters","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"Parameter Type Default Description\nfinish_callback Function — Called with (algorithm, carrier) when max_iter is reached\nconsensus_actor ConsensusActor — Provides the local gradient term; use NoConsensusActor() for pure averaging\ninitial_λ Real 10 Initial value broadcast to all components of λ\nα Real 0.3 Mixing step size\nmax_iter Int 50 Number of gossip rounds before finishing","category":"page"},{"location":"algorithms/consensus/#Local-Gradient-Corrections","page":"Averaging Consensus","title":"Local Gradient Corrections","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"To steer the consensus, implement a custom ConsensusActor and override gradient_term:","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"using DistributedResourceOptimization\n\n# Actor that pushes λ towards a local target\nstruct MyActor <: ConsensusActor\n    target::Vector{Float64}\n    β::Float64   # gradient step size\nend\n\nfunction gradient_term(actor::MyActor, λ::Vector{<:Real}, ::Any)\n    return actor.β .* (actor.target .- λ)\nend","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"The data argument carries whatever was embedded in the initial AveragingConsensusMessage — useful for passing problem data alongside the consensus.","category":"page"},{"location":"algorithms/consensus/#Complete-Example-—-Pure-Averaging","page":"Averaging Consensus","title":"Complete Example — Pure Averaging","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"using DistributedResourceOptimization\n\n# Three agents starting at λ = 5, 10, 15 will converge to their average (≈ 10)\nactor1 = create_averaging_consensus_participant((_, _) -> nothing, NoConsensusActor();\n             initial_λ=5.0, α=0.5, max_iter=30)\nactor2 = create_averaging_consensus_participant((_, _) -> nothing, NoConsensusActor();\n             initial_λ=10.0, α=0.5, max_iter=30)\nactor3 = create_averaging_consensus_participant((_, _) -> nothing, NoConsensusActor();\n             initial_λ=15.0, α=0.5, max_iter=30)\n\n# The start message sets the λ dimension (1-D here); initial=true triggers setup\nstart_msg = AveragingConsensusMessage([0.0], 0, nothing, true)\n\nwait(start_distributed_optimization([actor1, actor2, actor3], start_msg))\n\n# Gossip rounds run asynchronously — wait until actor1 has finished all iterations\nwait(Threads.@spawn while actor1.k < 30; sleep(0.01); end)\n\nprintln(\"actor1 λ ≈ \", round.(actor1.λ; digits=3))  # converges to ≈ [10.0]\nprintln(\"actor2 λ ≈ \", round.(actor2.λ; digits=3))\nprintln(\"actor3 λ ≈ \", round.(actor3.λ; digits=3))","category":"page"},{"location":"algorithms/consensus/#Complete-Example-—-Economic-Dispatch","page":"Averaging Consensus","title":"Complete Example — Economic Dispatch","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"The built-in LinearCostEconomicDispatchConsensusActor implements consensus-based economic dispatch, where each agent has a linear cost and power limits:","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"using DistributedResourceOptimization\n\n# TODO: example to be documented once LinearCostEconomicDispatchConsensusActor API is stable","category":"page"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"note: Termination\nThe algorithm terminates after exactly max_iter gossip rounds — there is no residual-based stopping criterion. Choose max_iter large enough for your network topology; in a fully connected graph convergence is typically fast (10–30 rounds).","category":"page"},{"location":"algorithms/consensus/#See-Also","page":"Averaging Consensus","title":"See Also","text":"","category":"section"},{"location":"algorithms/consensus/","page":"Averaging Consensus","title":"Averaging Consensus","text":"create_averaging_consensus_participant, AveragingConsensusAlgorithm\nConsensusActor, NoConsensusActor, gradient_term\nLinearCostEconomicDispatchConsensusActor","category":"page"},{"location":"carrier/mango/#MangoCarrier","page":"MangoCarrier","title":"MangoCarrier","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"The Mango carrier integrates DRO algorithms with Mango.jl, a Julia agent framework for realistic distributed system simulation. It enables multi-agent setups with TCP networking, making it suitable for scenarios that closely mirror real distributed deployments.","category":"page"},{"location":"carrier/mango/#Prerequisites","page":"MangoCarrier","title":"Prerequisites","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"Install Mango.jl alongside DRO:","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"using Pkg\nPkg.add([\"DistributedResourceOptimization\", \"Mango\"])","category":"page"},{"location":"carrier/mango/#Core-Concepts","page":"MangoCarrier","title":"Core Concepts","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"DRO integrates with Mango through two roles that can be assigned to Mango agents:","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"Role Purpose\nDistributedOptimizationRole Wraps a DistributedAlgorithm and handles incoming optimization messages\nCoordinatorRole Wraps a Coordinator and handles coordination messages (e.g., for ADMM)","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"A role internally creates a MangoCarrier that implements DRO's Carrier interface, delegating all message sends to Mango's messaging infrastructure.","category":"page"},{"location":"carrier/mango/#Basic-Setup-—-Distributed-Algorithm-(COHDA)","page":"MangoCarrier","title":"Basic Setup — Distributed Algorithm (COHDA)","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"using Mango, DistributedResourceOptimization\n\n# Create a Mango TCP container\ncontainer = create_tcp_container(\"127.0.0.1\", 5555)\n\n# Add two agents, each wrapping a COHDA participant\nagent_one = add_agent_composed_of(container,\n    DistributedOptimizationRole(\n        create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])))\n\nagent_two = add_agent_composed_of(container,\n    DistributedOptimizationRole(\n        create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])))\n\n# Wire agents into a fully-connected topology\nauto_assign!(complete_topology(2), container)\n\nstart_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\n# Activate the container and fire the start message\nactivate(container) do\n    send_message(agent_one, start_msg, address(agent_two))\nend","category":"page"},{"location":"carrier/mango/#Coordinated-Algorithm-(ADMM)","page":"MangoCarrier","title":"Coordinated Algorithm (ADMM)","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"For algorithms that require a coordinator, add a CoordinatorRole to an additional agent:","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"using Mango, DistributedResourceOptimization\n\ncontainer = create_tcp_container(\"127.0.0.1\", 5556)\n\n# Participant agents\nagent_flex1 = add_agent_composed_of(container,\n    DistributedOptimizationRole(create_admm_flex_actor_one_to_many(10.0, [0.1, 0.5, -1.0])))\n\nagent_flex2 = add_agent_composed_of(container,\n    DistributedOptimizationRole(create_admm_flex_actor_one_to_many(15.0, [0.1, 0.5, -1.0])))\n\n# Coordinator agent\nagent_coord = add_agent_composed_of(container,\n    CoordinatorRole(create_sharing_target_distance_admm_coordinator()))\n\n# Topology: participants know each other and the coordinator\nauto_assign!(complete_topology(3), container)\n\nstart_msg = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\n\nactivate(container) do\n    send_message(agent_coord, StartCoordinatedDistributedOptimization(start_msg),\n                 address(agent_coord))\nend","category":"page"},{"location":"carrier/mango/#Topology-Management","page":"MangoCarrier","title":"Topology Management","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"Mango.jl's complete_topology(N) creates a fully-connected graph for N agents. auto_assign! distributes the topology across the agents in the container so each agent knows the addresses of its neighbors.","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"For custom topologies, manually set the neighbors inside the role after construction, or use Mango.jl's topology utilities directly.","category":"page"},{"location":"carrier/mango/#Result-Notification","page":"MangoCarrier","title":"Result Notification","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"When a coordinated optimization finishes, the coordinator broadcasts an OptimizationFinishedMessage to all participants. Subscribe to it in your agent if you need to collect results:","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"using Mango, DistributedResourceOptimization\n\n@agent struct ResultCollector\n    results::Vector{Any}\nend\n\n@handle_message function collect(agent::ResultCollector, msg::OptimizationFinishedMessage, ::Any)\n    push!(agent.results, msg.result)\nend","category":"page"},{"location":"carrier/mango/#Comparison-with-SimpleCarrier","page":"MangoCarrier","title":"Comparison with SimpleCarrier","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"Feature SimpleCarrier MangoCarrier\nSetup complexity Minimal Moderate (Mango agents + container)\nCommunication In-process tasks TCP sockets\nSuitable for Unit tests, quick experiments Realistic distributed simulations\nParallelism Julia task scheduler Mango event loop\nTopology control Implicit (all in container) Explicit (Mango topology API)","category":"page"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"tip: When to Use MangoCarrier\nUse MangoCarrier when you need realistic network latency simulation, multi-process or multi-machine deployment, or integration with a larger Mango.jl agent system. For standalone algorithm experiments, SimpleCarrier is simpler and faster.","category":"page"},{"location":"carrier/mango/#See-Also","page":"MangoCarrier","title":"See Also","text":"","category":"section"},{"location":"carrier/mango/","page":"MangoCarrier","title":"MangoCarrier","text":"DistributedOptimizationRole, CoordinatorRole, MangoCarrier\nOptimizationFinishedMessage, StartCoordinatedDistributedOptimization\nSimpleCarrier for lightweight in-process simulations","category":"page"},{"location":"algorithms/admm/#ADMM","page":"ADMM","title":"ADMM","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The Alternating Direction Method of Multipliers (ADMM) is a convex optimization algorithm that decomposes a large problem into smaller subproblems solved locally, coordinated by a central update. DRO implements two ADMM variants: Sharing and Consensus.","category":"page"},{"location":"algorithms/admm/#Problem-Forms","page":"ADMM","title":"Problem Forms","text":"","category":"section"},{"location":"algorithms/admm/#Sharing","page":"ADMM","title":"Sharing","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The sharing form distributes a resource across N agents whose individual contributions must sum to a global variable z:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"beginequation\nbeginsplit\nmin_x_iz sum_i=1^N f_i(x_i) + g(z) \nquadtextstquad sum_i=1^N x_i = z\nendsplit\nendequation","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"where f_i is the local cost of agent i, x_i in mathbbR^m its decision variable, and g a global penalty on the aggregate z.","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The ADMM iterations with dual variable u and penalty rho are:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"beginalign\nx_i^k+1\n  = argmin_x_i\n     f_i(x_i) + tfracrho2biglVert x_i - (z^k - u^k) bigrVert_2^2\n     quad i=1dotsN\n     4pt\nbarx^k+1\n  = tfrac1Nsum_i=1^N x_i^k+1\n     4pt\nz^k+1\n  = argmin_z\n     g(Ncdot z) + tfracNrho2biglVert z - barx^k+1 - u^k bigrVert_2^2\n     4pt\nu^k+1\n  = u^k + barx^k+1 - z^k+1\nendalign","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"To create a coordinator for this form use create_sharing_target_distance_admm_coordinator, and start the negotiation with create_admm_start.","category":"page"},{"location":"algorithms/admm/#Consensus","page":"ADMM","title":"Consensus","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The consensus form drives all agents to agree on a single global value z:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"beginequation\nbeginsplit\nmin_x_iz sum_i=1^N f_i(x_i) \nquadtextstquad x_i = z i=1dotsN\nendsplit\nendequation","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The update iterations are:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"beginalign\nx_i^k+1\n  = argmin_x_i\n     f_i(x_i) + fracrho2big x_i - big(z^k - u_i^k big) big_2^2\n     4pt\nz^k+1\n  = argmin_z\n     g(z) + fracN rho2left\n     z - Big( barx^k+1 + baru^k Big)\n     right_2^2 4pt\nu_i^k+1\n  = u_i^k + x_i^k+1 - z^k+1\nendalign","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"To create a coordinator for the consensus form use create_consensus_target_reach_admm_coordinator, and to construct the start message use create_admm_start_consensus.","category":"page"},{"location":"algorithms/admm/#Local-Model:-Flexibility-Actor","page":"ADMM","title":"Local Model: Flexibility Actor","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"Each participant is modelled as a flexibility actor — a local resource with bounded and coupled decision variables:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"Constraint Description\nl_i leq x_i leq u_i Box constraints (lower/upper bounds per sector)\nC_i x_i leq d_i Coupling constraints (e.g., input-output coupling)\nS_i^top x_i Linear priority penalty added to the local objective","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"At each ADMM iteration the actor solves a small QP (via OSQP) to compute x_i^k+1.","category":"page"},{"location":"algorithms/admm/#One-to-Many-Resource","page":"ADMM","title":"One-to-Many Resource","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"A common model is a resource that converts a single input into m outputs with given efficiencies eta in mathbbR^m. Use create_admm_flex_actor_one_to_many:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"# 10 kW input capacity, three outputs with efficiencies [0.1, 0.5, -1.0]\n# Negative efficiency means the resource *consumes* that output type.\nactor = create_admm_flex_actor_one_to_many(10.0, [0.1, 0.5, -1.0])","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"An optional priorities vector P biases the solution towards specific sectors:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"# Prefer sector 1 with priority 5\nactor = create_admm_flex_actor_one_to_many(10.0, [0.1, 0.5, -1.0], [5.0, 0.0, 0.0])","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"After the optimization finishes, retrieve the result with:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"x_opt = result(actor)   # Vector{Float64} of length m","category":"page"},{"location":"algorithms/admm/#Coordinator-Parameters","page":"ADMM","title":"Coordinator Parameters","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"The generic ADMM coordinator (ADMMGenericCoordinator) exposes several tuning parameters:","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"Parameter Default Description\nρ 1.0 Penalty parameter — larger values enforce constraints faster but may slow convergence\nmax_iters 1000 Maximum number of ADMM iterations\nabs_tol 1e-4 Absolute primal/dual residual tolerance\nrel_tol 1e-3 Relative primal/dual residual tolerance","category":"page"},{"location":"algorithms/admm/#Complete-Example-—-ADMM-Sharing","page":"ADMM","title":"Complete Example — ADMM Sharing","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"using DistributedResourceOptimization\n\n# Three flexible resources (e.g., heat pumps, battery, PV inverter)\n# Each converts 10/15/10 kW input into three output types\nflex1 = create_admm_flex_actor_one_to_many(10.0, [0.1,  0.5, -1.0])\nflex2 = create_admm_flex_actor_one_to_many(15.0, [0.1,  0.5, -1.0])\nflex3 = create_admm_flex_actor_one_to_many(10.0, [-1.0, 0.0,  1.0])\n\n# Coordinator minimises weighted distance of Σxᵢ to target [-4, 0, 6]\n# Priority weights [5, 1, 1] penalise deviations in sector 1 most heavily\ncoordinator = create_sharing_target_distance_admm_coordinator()\nstart_msg   = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\n\nstart_coordinated_optimization([flex1, flex2, flex3], coordinator, start_msg)\n\nprintln(result(flex1))\nprintln(result(flex2))\nprintln(result(flex3))","category":"page"},{"location":"algorithms/admm/#Complete-Example-—-ADMM-Consensus","page":"ADMM","title":"Complete Example — ADMM Consensus","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"using DistributedResourceOptimization\n\n# Two flex actors converge to a common 2-dimensional target [1.0, 2.0].\n# Each actor has 10 kW input capacity with efficiency vector [0.6, 0.4].\nactor1 = create_admm_flex_actor_one_to_many(10.0, [0.6, 0.4])\nactor2 = create_admm_flex_actor_one_to_many(10.0, [0.6, 0.4])\n\ncoordinator = create_consensus_target_reach_admm_coordinator()\nstart_msg   = create_admm_start_consensus([1.0, 2.0])\n\nstart_coordinated_optimization([actor1, actor2], coordinator, start_msg)\n\nprintln(result(actor1))\nprintln(result(actor2))","category":"page"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"tip: Convergence Tips\nIf ADMM diverges or converges slowly, try:Reducing ρ when primal residuals dominate\nIncreasing ρ when dual residuals dominate\nTightening abs_tol / rel_tol for higher precision\nIncreasing max_iters if the warning \"reached max iterations\" appears","category":"page"},{"location":"algorithms/admm/#See-Also","page":"ADMM","title":"See Also","text":"","category":"section"},{"location":"algorithms/admm/","page":"ADMM","title":"ADMM","text":"ADMMFlexActor, create_admm_flex_actor_one_to_many, result\ncreate_sharing_target_distance_admm_coordinator, create_admm_start, create_admm_sharing_data\ncreate_consensus_target_reach_admm_coordinator, create_admm_start_consensus\nTutorial: Energy Dispatch with ADMM","category":"page"},{"location":"howtos/custom_algorithm/#How-To:-Implement-a-Custom-Algorithm","page":"Custom Algorithm","title":"How To: Implement a Custom Algorithm","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"DRO's algorithm abstraction has a single required method. Implementing it lets your algorithm participate in any carrier — SimpleCarrier, MangoCarrier, or one you write yourself.","category":"page"},{"location":"howtos/custom_algorithm/#The-Interface","page":"Custom Algorithm","title":"The Interface","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"All distributed algorithms extend DistributedAlgorithm and implement on_exchange_message:","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"abstract type DistributedAlgorithm end\n\nfunction on_exchange_message(\n    ::DistributedAlgorithm,\n    ::Carrier,\n    ::Any,   # message_data\n    ::Any,   # meta\n)\n    # Your logic here\nend","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Argument Type Description\nalgorithm Your concrete subtype Holds all algorithm state\ncarrier Carrier Use this to send replies or schedule callbacks\nmessage_data Any The deserialized message payload\nmeta Any Transport metadata (e.g., Dict(:sender => id) for SimpleCarrier)","category":"page"},{"location":"howtos/custom_algorithm/#Step-by-Step-Example-—-Echo-Algorithm","page":"Custom Algorithm","title":"Step-by-Step Example — Echo Algorithm","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"As a minimal example, here is an algorithm that counts how many messages it receives and echoes each one back to the sender.","category":"page"},{"location":"howtos/custom_algorithm/#.-Define-the-State-Struct","page":"Custom Algorithm","title":"1. Define the State Struct","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"using DistributedResourceOptimization\n\nmutable struct EchoAlgorithm <: DistributedAlgorithm\n    count::Int\n    EchoAlgorithm() = new(0)\nend","category":"page"},{"location":"howtos/custom_algorithm/#.-Define-a-Message-Type","page":"Custom Algorithm","title":"2. Define a Message Type","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Subtyping OptimizationMessage lets DRO identify your messages as part of an optimization session (required for correct dispatch through the carrier):","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"struct EchoMessage <: OptimizationMessage\n    payload::String\nend","category":"page"},{"location":"howtos/custom_algorithm/#.-Implement-on_exchange_message","page":"Custom Algorithm","title":"3. Implement on_exchange_message","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"function on_exchange_message(\n    algo::EchoAlgorithm,\n    carrier::Carrier,\n    msg::EchoMessage,\n    meta::Any,\n)\n    algo.count += 1\n    @info \"Received message #$(algo.count): $(msg.payload)\"\n\n    # Reply to sender with a confirmation\n    reply_to_other(carrier, EchoMessage(\"ACK: $(msg.payload)\"), meta)\nend","category":"page"},{"location":"howtos/custom_algorithm/#.-Run-it","page":"Custom Algorithm","title":"4. Run it","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"using DistributedResourceOptimization\n\nalgo1 = EchoAlgorithm()\nalgo2 = EchoAlgorithm()\n\ncontainer = ActorContainer()\ncarrier1  = SimpleCarrier(container, algo1)\ncarrier2  = SimpleCarrier(container, algo2)\n\nwait(send_to_other(carrier1, EchoMessage(\"hello\"), cid(carrier2)))","category":"page"},{"location":"howtos/custom_algorithm/#Coordinated-Algorithms","page":"Custom Algorithm","title":"Coordinated Algorithms","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"For algorithms that require a central coordinator (like ADMM), implement a Coordinator subtype alongside your DistributedAlgorithm:","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"abstract type Coordinator end\n\nfunction start_optimization(\n    ::Coordinator,\n    ::Carrier,\n    ::Any,   # message_data\n    ::Any,   # meta\n)\n    # Kick off the coordinated optimization\nend","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"The coordinator is wrapped in its own SimpleCarrier (or Mango agent) and is the entry point for start_coordinated_optimization.","category":"page"},{"location":"howtos/custom_algorithm/#Practical-Tips","page":"Custom Algorithm","title":"Practical Tips","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Keep algorithm state in the struct. Since each SimpleCarrier runs in separate Julia tasks, mutable fields of your algorithm struct are effectively actor state. Julia's task scheduler handles concurrent access within a single carrier.","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Use send_awaitable for request/reply patterns. If your algorithm needs a response before proceeding (e.g., a parallel x-update phase), use send_awaitable + wait:","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"event = send_awaitable(carrier, MyRequest(data), target_id)\nresponse = wait(carrier, event)","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Schedule callbacks with schedule_using. To trigger an action after a delay without blocking the current message handler:","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"schedule_using(carrier, () -> my_timeout_action(algo, carrier), 5.0)","category":"page"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"Termination. DRO has no built-in termination protocol — implement convergence detection inside your on_exchange_message and simply stop sending messages when done.","category":"page"},{"location":"howtos/custom_algorithm/#See-Also","page":"Custom Algorithm","title":"See Also","text":"","category":"section"},{"location":"howtos/custom_algorithm/","page":"Custom Algorithm","title":"Custom Algorithm","text":"DistributedAlgorithm, on_exchange_message\nCoordinator, start_optimization\nHow To: Implement a Custom Carrier\nCOHDA source for a real-world reference implementation","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Complete documentation for all exported types and functions.","category":"page"},{"location":"api/#Core-Abstractions","page":"API Reference","title":"Core Abstractions","text":"","category":"section"},{"location":"api/#Algorithm-Interface","page":"API Reference","title":"Algorithm Interface","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/core.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.CoordinatedDistributedAlgorithm","page":"API Reference","title":"DistributedResourceOptimization.CoordinatedDistributedAlgorithm","text":"CoordinatedDistributedAlgorithm\n\nA struct representing the core of a coordinated distributed optimization algorithm. This type encapsulates the necessary data and methods for coordinating multiple agents or processes in a distributed resource optimization setting.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.Coordinator","page":"API Reference","title":"DistributedResourceOptimization.Coordinator","text":"Coordinator\n\nAn abstract type representing the coordinator in distributed resource optimization algorithms. Concrete implementations of this type are responsible for managing and coordinating the optimization process across distributed resources.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.DistributedAlgorithm","page":"API Reference","title":"DistributedResourceOptimization.DistributedAlgorithm","text":"DistributedAlgorithm\n\nAn abstract type representing the base for distributed optimization algorithms. Subtypes should implement specific distributed algorithm logic for resource optimization.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.on_exchange_message-Tuple{DistributedAlgorithm, Carrier, Any, Any}","page":"API Reference","title":"DistributedResourceOptimization.on_exchange_message","text":"on_exchange_message(algorithm::DistributedAlgorithm, carrier::Carrier, message_data::Any, meta::Any)\n\nHandles an incoming exchange message within the distributed algorithm framework.\n\nArguments\n\nalgorithm::DistributedAlgorithm: The distributed algorithm instance managing the exchange.\ncarrier::Carrier: The communication carrier responsible for message delivery.\nmessage_data::Any: The data contained in the received message.\nmeta::Any: Additional metadata associated with the message.\n\nDescription\n\nProcesses the received exchange message, updating the algorithm's state or triggering appropriate actions based on the message content and metadata.\n\nReturns\n\nMay return a result or perform side effects depending on the implementation.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.start_optimization-Tuple{Coordinator, Carrier, Any, Any}","page":"API Reference","title":"DistributedResourceOptimization.start_optimization","text":"start_optimization(coordinator::Coordinator, carrier::Carrier, message_data::Any, meta::Any)\n\nInitiates the optimization process using the provided coordinator and carrier objects.  The function takes arbitrary message_data and meta information to configure or inform the optimization run.\n\nArguments\n\ncoordinator::Coordinator: The main coordinator responsible for managing the optimization workflow.\ncarrier::Carrier: The carrier object that facilitates communication or data transfer during optimization.\nmessage_data::Any: Additional data or messages required for the optimization process.\nmeta::Any: Communication Metadata or supplementary information relevant to the optimization.\n\nReturns\n\nThe result of the optimization process, which may vary depending on the implementation.\n\nNotes\n\nEnsure that coordinator and carrier are properly initialized before calling this function.\nThe types of message_data and meta are flexible to accommodate various use cases.\n\n\n\n\n\n","category":"method"},{"location":"api/#Carrier-Interface","page":"API Reference","title":"Carrier Interface","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"carrier/core.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.Carrier","page":"API Reference","title":"DistributedResourceOptimization.Carrier","text":"Carrier\n\nAn abstract type representing a generic data/communication carrier in the system. Concrete subtypes should implement specific carrier behaviors and properties.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.wait-Tuple{Carrier, Any}","page":"API Reference","title":"Base.wait","text":"Base.wait(carrier::Carrier, waitable::Any)\n\nWaits for the specified waitable object to become ready or complete within the context of the given carrier.  This function integrates with the Carrier's scheduling or resource management logic to handle asynchronous or blocking operations.\n\nArguments\n\ncarrier::Carrier: The carrier instance managing the waiting operation.\nwaitable::Any: The object or resource to wait for. This can be any type that supports waiting semantics.\n\nNotes\n\nThe function blocks until the waitable is ready or the operation is complete.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.others-Tuple{Carrier, String}","page":"API Reference","title":"DistributedResourceOptimization.others","text":"others(carrier::Carrier, participant_id::String)\n\nReturns a collection of participants in the given carrier excluding the participant with the specified participant_id.\n\nArguments\n\ncarrier::Carrier: The carrier object containing participants.\nparticipant_id::String: The identifier of the participant to exclude.\n\nReturns\n\nA collection (e.g., array or set) of participants other than the one with participant_id.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.reply_to_other-Tuple{Carrier, Any, Any}","page":"API Reference","title":"DistributedResourceOptimization.reply_to_other","text":"reply_to_other(carrier::Carrier, content_data::Any, meta::Any)\n\nReply using the given carrier to another entity, using the provided content_data and  meta information.\n\nArguments\n\ncarrier::Carrier: The carrier object responsible for communication.\ncontent_data::Any: The data to be sent in the reply.\nmeta::Any: Additional metadata associated with the reply.\n\nReturns\n\nThe result of the reply operation, depending on the implementation.\n\nNotes\n\nThe specific behavior depends on the implementation details of the Carrier type.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.schedule_using-Tuple{Carrier, Function, Float64}","page":"API Reference","title":"DistributedResourceOptimization.schedule_using","text":"schedule_using(carrier::Carrier, to_be_scheduled::Function, delay_s::Float64)\n\nSchedules the execution of the provided function to_be_scheduled using the specified carrier after a delay of delay_s seconds.\n\nArguments\n\ncarrier::Carrier: The carrier object responsible for scheduling.\nto_be_scheduled::Function: The function to be executed after the delay.\ndelay_s::Float64: The delay in seconds before executing the function.\n\nReturns\n\nImplementation-dependent. Typically, returns a handle or status indicating the scheduled task.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.send_awaitable-Tuple{Carrier, Any, Any}","page":"API Reference","title":"DistributedResourceOptimization.send_awaitable","text":"send_awaitable(carrier::Carrier, content_data::Any, receiver::Any)\n\nSends an awaitable message using the specified carrier to the given receiver with the provided content_data.\n\nArguments\n\ncarrier::Carrier: The carrier object responsible for message transmission.\ncontent_data::Any: The data to be sent in the message.\nreceiver::Any: The target recipient of the message.\n\nReturns\n\nReturns an awaitable object or handle that can be used to track the completion or response of the sent message.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.send_to_other-Tuple{Carrier, Any, Any}","page":"API Reference","title":"DistributedResourceOptimization.send_to_other","text":"send_to_other(carrier::Carrier, content_data::Any, receiver::Any)\n\nSends content_data using the specified carrier to the given receiver.\n\nArguments\n\ncarrier::Carrier: The carrier object responsible for sending the data.\ncontent_data::Any: The data to be sent.\nreceiver::Any: The target receiver of the data.\n\nReturns\n\nThe result of the send operation, which may vary depending on the implementation.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#Carriers","page":"API Reference","title":"Carriers","text":"","category":"section"},{"location":"api/#SimpleCarrier","page":"API Reference","title":"SimpleCarrier","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"carrier/simple.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.ActorContainer","page":"API Reference","title":"DistributedResourceOptimization.ActorContainer","text":"ActorContainer  \n\nA container to manage multiple `SimpleCarrier`.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.SimpleCarrier","page":"API Reference","title":"DistributedResourceOptimization.SimpleCarrier","text":"SimpleCarrier\n\nA concrete implementation of the Carrier type representing a simple carrier for distributed resource optimization. This carrier facilitates communication and scheduling among distributed algorithms within an ActorContainer.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.cid-Tuple{SimpleCarrier}","page":"API Reference","title":"DistributedResourceOptimization.cid","text":"cid(carrier::SimpleCarrier)\nreturn the carrier id\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.start_coordinated_optimization-Tuple{Vector{<:DistributedAlgorithm}, Coordinator, Any}","page":"API Reference","title":"DistributedResourceOptimization.start_coordinated_optimization","text":"start_coordinated_optimization(actors::Vector{<:DistributedAlgorithm}, coordinator::Coordinator, start_message::Any)\n\nStart a coordinated optimization process among the provided actors and a coordinator using the given start_message. Return the result of the optimization process. The coordinator manages the overall optimization flow.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.start_distributed_optimization-Tuple{Vector{<:DistributedAlgorithm}, Any}","page":"API Reference","title":"DistributedResourceOptimization.start_distributed_optimization","text":"start_distributed_optimization(actors::Vector{<:DistributedAlgorithm}, start_message::Any)\n\nStart a distributed optimization process among the provided actors using the given start_message. Return a waitable object that can be used to monitor the progress of the optimization.\n\n\n\n\n\n","category":"method"},{"location":"api/#MangoCarrier","page":"API Reference","title":"MangoCarrier","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"carrier/mango.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.CoordinatorRole","page":"API Reference","title":"DistributedResourceOptimization.CoordinatorRole","text":"CoordinatorRole\n\nA role struct representing the coordinator in the distributed resource optimization system. Used to identify and manage coordinator-specific logic and responsibilities within the carrier module.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.CoordinatorRole-Tuple{Coordinator}","page":"API Reference","title":"DistributedResourceOptimization.CoordinatorRole","text":"CoordinatorRole(coordinator::Coordinator; include_self=false, tid::Symbol = :default)\n\nAssigns the coordinator role to the specified coordinator object. \n\nArguments\n\ncoordinator::Coordinator: The coordinator instance to assign the role to.\ninclude_self::Bool=false: If true, includes the coordinator itself in the role assignment.\ntid::Symbol=:default: An optional identifier symbol for neighbor lookups.\n\nReturns\n\nReturns the configured coordinator role.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.DistributedOptimizationRole","page":"API Reference","title":"DistributedResourceOptimization.DistributedOptimizationRole","text":"DistributedOptimizationRole\n\nA role struct used to define distributed optimization responsibilities within the system. Attach this role to entities that participate in distributed resource optimization processes.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.DistributedOptimizationRole-Tuple{DistributedAlgorithm}","page":"API Reference","title":"DistributedResourceOptimization.DistributedOptimizationRole","text":"DistributedOptimizationRole(algorithm::DistributedAlgorithm; tid::Symbol = :default)\n\nCreates and returns a distributed optimization role using the specified algorithm.  An optional identifier tid can be provided, defaulting to :default.\n\nArguments\n\nalgorithm::DistributedAlgorithm: The distributed optimization algorithm to be used.\ntid::Symbol: (Optional) Identifier for the neighbor lookup. Defaults to :default.\n\nReturns\n\nA distributed optimization role configured with the given algorithm and an identifier to specify the addresses of other participants.\n\n\n\n\n\n","category":"method"},{"location":"api/#DistributedResourceOptimization.MangoCarrier","page":"API Reference","title":"DistributedResourceOptimization.MangoCarrier","text":"MangoCarrier <: Carrier\n\nA concrete implementation of the Carrier type representing a mango carrier. Extend this struct with relevant fields and methods to model the behavior and properties of a mango carrier in the distributed resource optimization context.\n\n\n\n\n\n","category":"type"},{"location":"api/#DistributedResourceOptimization.OptimizationFinishedMessage","page":"API Reference","title":"DistributedResourceOptimization.OptimizationFinishedMessage","text":"OptimizationFinishedMessage\n\nA message struct indicating that an optimization process has completed. Typically used for signaling the end of an optimization routine in distributed or parallel computation contexts.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#Algorithms","page":"API Reference","title":"Algorithms","text":"","category":"section"},{"location":"api/#ADMM-—-Core","page":"API Reference","title":"ADMM — Core","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/admm/core.jl\"]","category":"page"},{"location":"api/#ADMM-—-Sharing-Variant","page":"API Reference","title":"ADMM — Sharing Variant","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/admm/sharing_admm.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.create_sharing_admm_coordinator-Tuple{ADMMGlobalObjective}","page":"API Reference","title":"DistributedResourceOptimization.create_sharing_admm_coordinator","text":"create_sharing_admm_coordinator(objective::ADMMGlobalObjective)\n\nArguments\n\nobjective::ADMMGlobalObjective: The global objective function to be used in the AD\n\n\n\n\n\n","category":"method"},{"location":"api/#ADMM-—-Consensus-Variant","page":"API Reference","title":"ADMM — Consensus Variant","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/admm/consensus_admm.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.create_admm_start_consensus-Tuple{Vector{<:Real}}","page":"API Reference","title":"DistributedResourceOptimization.create_admm_start_consensus","text":"create_admm_start_consensus(target::Vector{<:Real})\n\nCreate an `ADMMStart` message for consensus ADMM with the specified target vector.\n\n\n\n\n\n","category":"method"},{"location":"api/#ADMM-—-Flexibility-Actor","page":"API Reference","title":"ADMM — Flexibility Actor","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/admm/flex_actor.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.create_admm_flex_actor_one_to_many","page":"API Reference","title":"DistributedResourceOptimization.create_admm_flex_actor_one_to_many","text":"create_admm_flex_actor_one_to_many(in_capacity::Real, η::Vector{Float64}, P::Union{Nothing,Vector{<:Real}}=nothing)\n\nCreates an ADMM flex actor for a one-to-many resource allocation scenario.\n\nArguments\n\nin_capacity::Real: The input capacity of the resource.\nη::Vector{Float64}: Vector of efficiency parameters for the ADMM algorithm.\nP::Union{Nothing,Vector{<:Real}}: Optional vector of priorities. If not provided, defaults to nothing.\n\nReturns\n\nA flex actor object configured for one-to-many ADMM optimization.\n\nNotes\n\nThis function is typically used in distributed resource optimization problems where a single resource is allocated to multiple consumers using the ADMM algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/#COHDA","page":"API Reference","title":"COHDA","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/heuristic/cohda/core.jl\",\n           \"algorithm/heuristic/cohda/decider.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.result-Tuple{COHDAAlgorithmData}","page":"API Reference","title":"DistributedResourceOptimization.result","text":"result(actor::COHDAAlgorithmData) -> Vector{Float64}\n\nReturn the aggregate schedule chosen by the COHDA optimization — the column-wise sum of all participants' schedules in the best solution candidate known to this actor.\n\n\n\n\n\n","category":"method"},{"location":"api/#Averaging-Consensus","page":"API Reference","title":"Averaging Consensus","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [DistributedResourceOptimization]\nPrivate = false\nPages   = [\"algorithm/consensus/averaging.jl\",\n           \"algorithm/consensus/economic_dispatch.jl\"]","category":"page"},{"location":"api/#DistributedResourceOptimization.create_averaging_consensus_start","page":"API Reference","title":"DistributedResourceOptimization.create_averaging_consensus_start","text":"create_averaging_consensus_start(initial_λ::Real, data::Any=nothing) -> AveragingConsensusMessage\n\nCreate the initial start message for averaging consensus. initial_λ sets the starting price/signal value (scalar, broadcast to all dimensions), and data is any auxiliary payload forwarded unchanged to each participant's gradient_term.\n\n\n\n\n\n","category":"function"},{"location":"carrier/simple/#SimpleCarrier","page":"SimpleCarrier","title":"SimpleCarrier","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"SimpleCarrier is DRO's built-in, lightweight carrier. It runs all participants as Julia tasks within a single process, with no network or serialization overhead. It is the recommended choice for prototyping, testing, and single-machine simulations.","category":"page"},{"location":"carrier/simple/#Core-Types","page":"SimpleCarrier","title":"Core Types","text":"","category":"section"},{"location":"carrier/simple/#ActorContainer","page":"SimpleCarrier","title":"ActorContainer","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"An ActorContainer holds all SimpleCarrier instances and lets them find each other by numeric ID. Create one container per simulation:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"using DistributedResourceOptimization\n\ncontainer = ActorContainer()","category":"page"},{"location":"carrier/simple/#SimpleCarrier-2","page":"SimpleCarrier","title":"SimpleCarrier","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"Wraps an algorithm (or coordinator) and registers it with a container:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"carrier_one = SimpleCarrier(container, create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]]))\ncarrier_two = SimpleCarrier(container, create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]]))","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"Each carrier is automatically assigned an integer ID when it registers. Retrieve it with cid:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"id_one = cid(carrier_one)   # => 1\nid_two = cid(carrier_two)   # => 2","category":"page"},{"location":"carrier/simple/#Sending-Messages","page":"SimpleCarrier","title":"Sending Messages","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"Use send_to_other to dispatch a message to another carrier in the same container. The message is delivered asynchronously (spawned as a Julia task):","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"start_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\n# Returns a task; wait for it to complete\nwait(send_to_other(carrier_one, start_msg, cid(carrier_two)))","category":"page"},{"location":"carrier/simple/#Express-API","page":"SimpleCarrier","title":"Express API","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"For quick experiments you can skip creating the container and carriers yourself. start_distributed_optimization wraps everything in a single call:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"using DistributedResourceOptimization\n\nactor_one = create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\nactor_two = create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\n\nstart_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\nwait(start_distributed_optimization([actor_one, actor_two], start_msg))","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"For coordinated algorithms (e.g., ADMM), use start_coordinated_optimization:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"using DistributedResourceOptimization\n\nflex1 = create_admm_flex_actor_one_to_many(10.0, [0.1, 0.5, -1.0])\nflex2 = create_admm_flex_actor_one_to_many(15.0, [0.1, 0.5, -1.0])\nflex3 = create_admm_flex_actor_one_to_many(10.0, [-1.0, 0.0, 1.0])\n\ncoordinator = create_sharing_target_distance_admm_coordinator()\nstart_msg   = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\n\nstart_coordinated_optimization([flex1, flex2, flex3], coordinator, start_msg)","category":"page"},{"location":"carrier/simple/#Awaitable-Messages","page":"SimpleCarrier","title":"Awaitable Messages","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"If a participant needs a response from another before continuing, use send_awaitable, which returns an EventWithValue that can be waited on:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"event = send_awaitable(carrier_one, my_request, cid(carrier_two))\nresponse = wait(carrier_one, event)","category":"page"},{"location":"carrier/simple/#Scheduling-Delayed-Calls","page":"SimpleCarrier","title":"Scheduling Delayed Calls","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"To execute a function after a delay (without blocking the caller), use schedule_using:","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"schedule_using(carrier_one, () -> println(\"fired after 1 s\"), 1.0)","category":"page"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"note: Thread Safety\nAll message dispatches are wrapped in [@spawnlog], a macro that spawns a Julia task and logs any exceptions with thread ID and backtrace. Make sure your Julia runtime has multiple threads available (julia -t auto) if you want true parallelism.","category":"page"},{"location":"carrier/simple/#See-Also","page":"SimpleCarrier","title":"See Also","text":"","category":"section"},{"location":"carrier/simple/","page":"SimpleCarrier","title":"SimpleCarrier","text":"SimpleCarrier, ActorContainer, cid\nstart_distributed_optimization, start_coordinated_optimization\nsend_to_other, send_awaitable, schedule_using\nMangoCarrier for agent-based simulations with TCP networking","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"DRO is registered in the Julia General registry. Open a Julia REPL and run:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"DistributedResourceOptimization\")","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To use the Mango.jl carrier for agent-based simulations, also add:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Pkg.add(\"Mango\")","category":"page"},{"location":"getting_started/#Choosing-an-Algorithm","page":"Getting Started","title":"Choosing an Algorithm","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"DRO currently provides three algorithm families:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Algorithm Problem type Coordination\nADMM Sharing Continuous, convex resource allocation Coordinator required\nADMM Consensus Continuous, consensus towards a shared target Coordinator required\nCOHDA Combinatorial schedule selection Fully distributed\nAveraging Consensus Distributed averaging with gradient terms Fully distributed","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use ADMM when you have flexible resources with bounded, continuous decision variables and you want to minimize a convex global objective (e.g., match a power target while minimizing deviation cost).","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use COHDA when each participant selects exactly one schedule from a discrete set and the goal is to minimize the distance of the combined schedule to a target.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use Averaging Consensus when you need distributed averaging across agents, optionally with local gradient corrections.","category":"page"},{"location":"getting_started/#Choosing-a-Carrier","page":"Getting Started","title":"Choosing a Carrier","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"A carrier handles the communication between participants. DRO separates the algorithm from the carrier, so you can switch backends without touching algorithm code.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Carrier When to use\nNo carrier (express API) Quick experiments, single-process simulation\nSimpleCarrier Multi-participant simulation in one process, full control over message flow\nMangoCarrier Realistic distributed simulations with TCP networking via Mango.jl","category":"page"},{"location":"getting_started/#Usage-Patterns","page":"Getting Started","title":"Usage Patterns","text":"","category":"section"},{"location":"getting_started/#Pattern-1-—-Express-API-(no-carrier-setup)","page":"Getting Started","title":"Pattern 1 — Express API (no carrier setup)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The simplest way to run a distributed optimization. DRO wraps everything in a SimpleCarrier internally. Best for quick experiments.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Distributed algorithms (COHDA, Averaging Consensus):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using DistributedResourceOptimization\n\n# Create two participants; each picks one schedule from two choices\nactor_one = create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\nactor_two = create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\n\n# Target combined schedule\nstart_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\n# Returns a waitable task; wait for convergence\nwait(start_distributed_optimization([actor_one, actor_two], start_msg))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Coordinated algorithms (ADMM):","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using DistributedResourceOptimization\n\n# Three resources, each with capacity and efficiency vector\nflex_actor1 = create_admm_flex_actor_one_to_many(10, [0.1, 0.5, -1.0])\nflex_actor2 = create_admm_flex_actor_one_to_many(15, [0.1, 0.5, -1.0])\nflex_actor3 = create_admm_flex_actor_one_to_many(10, [-1.0, 0.0,  1.0])\n\n# Coordinator solves the global z-update step\ncoordinator = create_sharing_target_distance_admm_coordinator()\n\n# Target power vector [-4, 0, 6] with priorities [5, 1, 1]\nstart_msg = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\n\nstart_coordinated_optimization([flex_actor1, flex_actor2, flex_actor3], coordinator, start_msg)","category":"page"},{"location":"getting_started/#Pattern-2-—-SimpleCarrier-(explicit-carrier-setup)","page":"Getting Started","title":"Pattern 2 — SimpleCarrier (explicit carrier setup)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use SimpleCarrier when you need direct control: custom message routing, result inspection, or integration with a larger system.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using DistributedResourceOptimization\n\n# Container holds all carriers and lets them find each other\ncontainer = ActorContainer()\n\nactor_one = SimpleCarrier(container, create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]]))\nactor_two = SimpleCarrier(container, create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]]))\n\nstart_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\n# Manually kick off the algorithm by sending the start message\nwait(send_to_other(actor_one, start_msg, cid(actor_two)))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use cid(carrier) to get the integer ID of a carrier within its container.","category":"page"},{"location":"getting_started/#Pattern-3-—-MangoCarrier-(agent-based-simulation)","page":"Getting Started","title":"Pattern 3 — MangoCarrier (agent-based simulation)","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"For realistic distributed simulations with TCP networking via Mango.jl. See the Mango carrier guide for full details.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Mango, DistributedResourceOptimization\n\ncontainer = create_tcp_container(\"127.0.0.1\", 5555)\n\nagent_one = add_agent_composed_of(container,\n    DistributedOptimizationRole(create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])))\nagent_two = add_agent_composed_of(container,\n    DistributedOptimizationRole(create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])))\n\nauto_assign!(complete_topology(2), container)\n\nstart_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\nactivate(container) do\n    send_message(agent_one, start_msg, address(agent_two))\nend","category":"page"},{"location":"getting_started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"See the Tutorials for complete, end-to-end examples\nRead the Algorithm pages for mathematical background and parameter guidance\nCheck the How-To Guides to implement your own algorithms or carriers\nBrowse the API Reference for all exported symbols","category":"page"},{"location":"algorithms/cohda/#COHDA","page":"COHDA","title":"COHDA","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"COHDA (Combinatorial Optimization Heuristic for Distributed Agents) is a fully distributed heuristic for the Multiple-Choice Combinatorial Optimization Problem (MC-COP). It requires no central coordinator — participants exchange solution candidates with their neighbors and converge through local search.","category":"page"},{"location":"algorithms/cohda/#Problem-Formulation","page":"COHDA","title":"Problem Formulation","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"Each of the N participants independently selects exactly one schedule from a personal menu of M choices. The goal is to minimize the L1 distance of the combined schedule sum to a target vector T:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"beginequation\n  beginsplit\n      undersetx_ijtextmaxleft(-bigllVert T - sum_i=1^Nsum_j=1^M U_ijcdot x_ijbigrrVert_1right)\n      textstquad sum_j=1^M x_ij = 1 quad forall i\n      phantomtextstquad x_ijin01quad i=1dotsN j=1dotsM\n  endsplit\nendequation","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"where U_ij in mathbbR^m is the j-th schedule of participant i.","category":"page"},{"location":"algorithms/cohda/#How-It-Works","page":"COHDA","title":"How It Works","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"COHDA is a gossip-style algorithm:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"A start message carrying the target T is sent to one participant.\nUpon receiving a message, each participant updates its working memory — a view of the current best known system configuration — and then applies local search to improve its own schedule selection.\nIf the working memory changed, the participant broadcasts an updated solution candidate to all its neighbors.\nThe algorithm terminates when no participant can improve the objective any further (the system configuration stabilises across all participants).","category":"page"},{"location":"algorithms/cohda/#Usage","page":"COHDA","title":"Usage","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"Create participants with create_cohda_participant:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"using DistributedResourceOptimization\n\n# Participant 1: two schedule choices, each 3-dimensional\nactor_one = create_cohda_participant(1, [[0.0, 1.0, 2.0],\n                                         [1.0, 2.0, 3.0]])\n\n# Participant 2: two schedule choices\nactor_two = create_cohda_participant(2, [[0.0, 1.0, 2.0],\n                                         [1.0, 2.0, 3.0]])","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"The first argument is a unique participant ID (integer); the second is a vector of schedules, each of which must have the same length as the target vector.","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"Start the optimization with a target vector using create_cohda_start_message:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"start_msg = create_cohda_start_message([1.2, 2.0, 3.0])\n\nwait(start_distributed_optimization([actor_one, actor_two], start_msg))","category":"page"},{"location":"algorithms/cohda/#Complete-Example","page":"COHDA","title":"Complete Example","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"using DistributedResourceOptimization\n\n# Four participants, each with three schedule options of length 4\nschedules_A = [[1.0, 0.0, 0.0, 0.0],\n               [0.0, 1.0, 0.0, 0.0],\n               [0.0, 0.0, 1.0, 0.0]]\n\nschedules_B = [[2.0, 0.0, 0.0, 0.0],\n               [0.0, 2.0, 0.0, 0.0],\n               [0.0, 0.0, 0.0, 2.0]]\n\nactors = [\n    create_cohda_participant(1, schedules_A),\n    create_cohda_participant(2, schedules_B),\n    create_cohda_participant(3, schedules_A),\n    create_cohda_participant(4, schedules_B),\n]\n\n# Target: the combined schedule should be close to [3, 3, 1, 2]\nstart_msg = create_cohda_start_message([3.0, 3.0, 1.0, 2.0])\n\nwait(start_distributed_optimization(actors, start_msg))","category":"page"},{"location":"algorithms/cohda/#Working-Memory-and-Solution-Candidates","page":"COHDA","title":"Working Memory and Solution Candidates","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"Internally COHDA uses:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"WorkingMemory — each participant's current view of the world: the target parameters, the best known system configuration (SystemConfig), and the best known solution candidate (SolutionCandidate).\nSystemConfig — a mapping from participant IDs to their currently selected schedule.\nSolutionCandidate — a proposed full system configuration together with its performance value (negative L1 distance to target).","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"These types are exported and can be inspected after the algorithm runs.","category":"page"},{"location":"algorithms/cohda/#Custom-Performance-Functions","page":"COHDA","title":"Custom Performance Functions","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"By default COHDA uses a weighted L1 distance metric. You can provide a custom performance function when creating a participant. The function receives a Matrix{Float64} (rows = participants, columns = time steps) and a TargetParams struct, and must return a Float64:","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"using DistributedResourceOptimization\n\nschedules_A = [[1.0, 0.0, 0.0, 0.0],\n               [0.0, 1.0, 0.0, 0.0],\n               [0.0, 0.0, 1.0, 0.0]]\n\n# Custom performance using L2 distance instead of the default weighted L1\nmy_perf = (cluster_schedule, target_params) ->\n    -sqrt(sum((target_params.schedule .- vec(sum(cluster_schedule, dims=1))) .^ 2))\n\nactor = create_cohda_participant(1, (_) -> schedules_A, my_perf)","category":"page"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"note: Convergence\nCOHDA is a heuristic and does not guarantee a globally optimal solution. Quality typically improves with more participants and more schedule choices. Convergence is detected when the system configuration stabilises across all participants.","category":"page"},{"location":"algorithms/cohda/#See-Also","page":"COHDA","title":"See Also","text":"","category":"section"},{"location":"algorithms/cohda/","page":"COHDA","title":"COHDA","text":"create_cohda_participant, create_cohda_start_message\nWorkingMemory, SolutionCandidate, SystemConfig\nTutorial: Schedule Coordination with COHDA","category":"page"},{"location":"tutorials/energy_dispatch/#Tutorial:-Energy-Dispatch-with-ADMM","page":"Energy Dispatch (ADMM)","title":"Tutorial: Energy Dispatch with ADMM","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"This tutorial walks through using ADMM to coordinate flexible energy resources — heat pumps, batteries, and PV inverters — so that their combined power output matches a target profile.","category":"page"},{"location":"tutorials/energy_dispatch/#Problem-Statement","page":"Energy Dispatch (ADMM)","title":"Problem Statement","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"Suppose an aggregator controls three flexible resources. Each resource converts an input power into outputs across three sectors (e.g., thermal, electrical, reactive power). The aggregator wants the combined output to be as close as possible to:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"target = [-4.0, 0.0, 6.0]  kW","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"with the first sector weighted five times more heavily than the others (e.g., it is a critical load).","category":"page"},{"location":"tutorials/energy_dispatch/#Step-1-—-Define-the-Resources","page":"Energy Dispatch (ADMM)","title":"Step 1 — Define the Resources","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"Each resource is modelled as an ADMM flex actor. The constructor create_admm_flex_actor_one_to_many takes:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"in_capacity — maximum input power in kW\nη — efficiency vector mapping input to outputs (negative means the resource consumes that output type)\nP (optional) — priority penalty vector that biases the local solution","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"using DistributedResourceOptimization\n\n# Resource 1: 10 kW heat pump — produces thermal (η₁=0.1) and electrical (η₂=0.5),\n#             consumes reactive power (η₃=−1.0)\nresource1 = create_admm_flex_actor_one_to_many(10.0, [0.1, 0.5, -1.0])\n\n# Resource 2: 15 kW battery — same conversion ratios, higher capacity\nresource2 = create_admm_flex_actor_one_to_many(15.0, [0.1, 0.5, -1.0])\n\n# Resource 3: 10 kW PV inverter — consumes thermal, neutral electrical, produces reactive\nresource3 = create_admm_flex_actor_one_to_many(10.0, [-1.0, 0.0, 1.0])","category":"page"},{"location":"tutorials/energy_dispatch/#Step-2-—-Create-the-Coordinator","page":"Energy Dispatch (ADMM)","title":"Step 2 — Create the Coordinator","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"The coordinator solves the global z-update each ADMM iteration. We use the sharing variant, which minimises the weighted distance of the aggregate output to the target:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"coordinator = create_sharing_target_distance_admm_coordinator()","category":"page"},{"location":"tutorials/energy_dispatch/#Step-3-—-Set-Up-the-Problem-Data","page":"Energy Dispatch (ADMM)","title":"Step 3 — Set Up the Problem Data","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"create_admm_sharing_data bundles the target vector and priority weights:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"# target = [-4, 0, 6], priorities = [5, 1, 1]\nproblem_data = create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1])\nstart_msg    = create_admm_start(problem_data)","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"The priority weight 5 for the first sector means deviations there are penalised five times more than deviations in the other two sectors.","category":"page"},{"location":"tutorials/energy_dispatch/#Step-4-—-Run-the-Optimization","page":"Energy Dispatch (ADMM)","title":"Step 4 — Run the Optimization","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"start_coordinated_optimization(\n    [resource1, resource2, resource3],\n    coordinator,\n    start_msg,\n)","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"This call blocks until the ADMM algorithm converges (or hits max_iters).","category":"page"},{"location":"tutorials/energy_dispatch/#Step-5-—-Read-the-Results","page":"Energy Dispatch (ADMM)","title":"Step 5 — Read the Results","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"After convergence, retrieve each resource's optimal output vector with result:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"x1 = result(resource1)\nx2 = result(resource2)\nx3 = result(resource3)\n\nprintln(\"Resource 1 output: \", round.(x1; digits=3))\nprintln(\"Resource 2 output: \", round.(x2; digits=3))\nprintln(\"Resource 3 output: \", round.(x3; digits=3))\nprintln(\"Aggregate:         \", round.(x1 .+ x2 .+ x3; digits=3))\nprintln(\"Target:            \", [-4.0, 0.0, 6.0])","category":"page"},{"location":"tutorials/energy_dispatch/#Complete-Script","page":"Energy Dispatch (ADMM)","title":"Complete Script","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"using DistributedResourceOptimization\n\n# Resources\nresource1 = create_admm_flex_actor_one_to_many(10.0, [0.1,  0.5, -1.0])\nresource2 = create_admm_flex_actor_one_to_many(15.0, [0.1,  0.5, -1.0])\nresource3 = create_admm_flex_actor_one_to_many(10.0, [-1.0, 0.0,  1.0])\n\n# Coordinator and problem\ncoordinator  = create_sharing_target_distance_admm_coordinator()\nproblem_data = create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1])\nstart_msg    = create_admm_start(problem_data)\n\n# Solve\nstart_coordinated_optimization([resource1, resource2, resource3], coordinator, start_msg)\n\n# Results\nx1, x2, x3 = result(resource1), result(resource2), result(resource3)\nprintln(\"Resource 1: \", round.(x1; digits=3))\nprintln(\"Resource 2: \", round.(x2; digits=3))\nprintln(\"Resource 3: \", round.(x3; digits=3))\nprintln(\"Aggregate:  \", round.(x1 .+ x2 .+ x3; digits=3))","category":"page"},{"location":"tutorials/energy_dispatch/#Tuning-Convergence","page":"Energy Dispatch (ADMM)","title":"Tuning Convergence","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"If the result is not accurate enough, or convergence is slow, build an ADMMGenericCoordinator directly to override any parameter:","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"using DistributedResourceOptimization\n\nresource1 = create_admm_flex_actor_one_to_many(10.0, [0.1,  0.5, -1.0])\nresource2 = create_admm_flex_actor_one_to_many(15.0, [0.1,  0.5, -1.0])\nresource3 = create_admm_flex_actor_one_to_many(10.0, [-1.0, 0.0,  1.0])\n\ncoordinator = ADMMGenericCoordinator(\n    global_actor = ADMMSharingGlobalActor(ADMMTargetDistanceObjective()),\n    ρ            = 5.0,    # larger ρ enforces constraints more aggressively\n    max_iters    = 500,    # allow more iterations\n    abs_tol      = 1e-5,   # tighter tolerance\n    rel_tol      = 1e-4,\n)\n\nstart_msg = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\nstart_coordinated_optimization([resource1, resource2, resource3], coordinator, start_msg)\n\nx1, x2, x3 = result(resource1), result(resource2), result(resource3)\nprintln(\"Aggregate:  \", round.(x1 .+ x2 .+ x3; digits=3))","category":"page"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"tip: Priority Weights\nSetting large priority weights for a sector (e.g., [100, 1, 1]) forces the optimizer to match that sector's target as closely as possible, potentially at the expense of other sectors. Use this to encode hard priorities in soft constraints.","category":"page"},{"location":"tutorials/energy_dispatch/#Next-Steps","page":"Energy Dispatch (ADMM)","title":"Next Steps","text":"","category":"section"},{"location":"tutorials/energy_dispatch/","page":"Energy Dispatch (ADMM)","title":"Energy Dispatch (ADMM)","text":"Try adding a fourth resource and observe how the aggregate adapts\nExperiment with different efficiency vectors η to model other resource types\nSee ADMM algorithm details for the mathematical background\nSee How To: Implement a Custom Algorithm to write your own local model","category":"page"},{"location":"#DistributedResourceOptimization.jl","page":"Home","title":"DistributedResourceOptimization.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRO is a Julia package for distributed optimization of resources across decentralized systems. It provides a clean collection of algorithms — ADMM, COHDA, and Averaging Consensus — paired with a carrier abstraction that decouples algorithm logic from the underlying communication layer.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"feature-grid\">\n  <div class=\"feature-card\">\n    <div class=\"feature-title\">Multiple Algorithms</div>\n    <div class=\"feature-desc\">ADMM (Sharing &amp; Consensus), COHDA heuristic, and Averaging Consensus — ready to use out of the box.</div>\n  </div>\n  <div class=\"feature-card\">\n    <div class=\"feature-title\">Pluggable Carriers</div>\n    <div class=\"feature-desc\">Swap communication backends without touching algorithm code. Built-in SimpleCarrier or full Mango.jl agent framework.</div>\n  </div>\n  <div class=\"feature-card\">\n    <div class=\"feature-title\">Async by Design</div>\n    <div class=\"feature-desc\">Algorithms communicate via asynchronous message passing, built on Julia's native task system.</div>\n  </div>\n  <div class=\"feature-card\">\n    <div class=\"feature-title\">Research-Ready</div>\n    <div class=\"feature-desc\">Mathematical formulations with configurable parameters. Easily extend with custom algorithms and carriers.</div>\n  </div>\n</div>","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DRO is registered in the Julia General registry. Install it with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"DistributedResourceOptimization\")","category":"page"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The simplest way to run a distributed optimization is using the express API — no carrier setup required:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DistributedResourceOptimization\n\n# COHDA: each participant selects one schedule to minimize distance to a target\nactor_one = create_cohda_participant(1, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\nactor_two = create_cohda_participant(2, [[0.0, 1.0, 2.0], [1.0, 2.0, 3.0]])\n\ntarget = create_cohda_start_message([1.2, 2.0, 3.0])\n\nwait(start_distributed_optimization([actor_one, actor_two], target))","category":"page"},{"location":"","page":"Home","title":"Home","text":"For coordinated algorithms like ADMM, a coordinator manages the global update:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DistributedResourceOptimization\n\n# Three flexible resources sharing capacity to reach a target\nflex_actor1 = create_admm_flex_actor_one_to_many(10, [0.1, 0.5, -1.0])\nflex_actor2 = create_admm_flex_actor_one_to_many(15, [0.1, 0.5, -1.0])\nflex_actor3 = create_admm_flex_actor_one_to_many(10, [-1.0, 0.0,  1.0])\n\ncoordinator = create_sharing_target_distance_admm_coordinator()\nstart_msg   = create_admm_start(create_admm_sharing_data([-4.0, 0.0, 6.0], [5, 1, 1]))\n\nstart_coordinated_optimization([flex_actor1, flex_actor2, flex_actor3], coordinator, start_msg)","category":"page"},{"location":"#What's-Included","page":"Home","title":"What's Included","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Category Name Description\nAlgorithm ADMM Sharing Distributed resource allocation with a shared global variable\nAlgorithm ADMM Consensus Consensus towards a common target value\nAlgorithm COHDA Combinatorial heuristic for schedule selection\nAlgorithm Averaging Consensus Gossip-based averaging with gradient terms\nCarrier SimpleCarrier Lightweight in-process carrier for prototyping and testing\nCarrier MangoCarrier Full agent-based carrier via Mango.jl","category":"page"},{"location":"#Navigation-Guide","page":"Home","title":"Navigation Guide","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<div class=\"nav-grid\">\n  <a class=\"nav-card\" href=\"getting_started/\">\n    <div class=\"nav-title\">Getting Started</div>\n    <div class=\"nav-desc\">Installation, first steps, and choosing an algorithm</div>\n  </a>\n  <a class=\"nav-card\" href=\"algorithms/admm/\">\n    <div class=\"nav-title\">Algorithms</div>\n    <div class=\"nav-desc\">Mathematical background and usage for each algorithm</div>\n  </a>\n  <a class=\"nav-card\" href=\"tutorials/energy_dispatch/\">\n    <div class=\"nav-title\">Tutorials</div>\n    <div class=\"nav-desc\">End-to-end walkthroughs for concrete use cases</div>\n  </a>\n  <a class=\"nav-card\" href=\"howtos/custom_algorithm/\">\n    <div class=\"nav-title\">How-To Guides</div>\n    <div class=\"nav-desc\">Implement your own algorithms and carriers</div>\n  </a>\n  <a class=\"nav-card\" href=\"carrier/simple/\">\n    <div class=\"nav-title\">Carriers</div>\n    <div class=\"nav-desc\">Choose and configure a communication backend</div>\n  </a>\n  <a class=\"nav-card\" href=\"api/\">\n    <div class=\"nav-title\">API Reference</div>\n    <div class=\"nav-desc\">Complete documentation of all exported functions and types</div>\n  </a>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Work in Progress\nDRO is under active development. APIs may change between minor versions. Feedback and contributions are welcome on GitHub.","category":"page"}]
}
